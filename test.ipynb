{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base.registry import RUNNER\n",
    "import torch.nn as nn\n",
    "\n",
    "@RUNNER.register_module()\n",
    "class Runner(nn.Module):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        print(self.a, self.b)\n",
    "        return self.a + self.b\n",
    "    \n",
    "runner = RUNNER.build(dict(type='Runner', a=1, b=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 480, 320])\n",
      "torch.Size([4, 1, 480, 320])\n"
     ]
    }
   ],
   "source": [
    "from base.registry import MY_DATASETS\n",
    "import os.path as osp\n",
    "root = osp.join('data', 'sonar')\n",
    "dataset = MY_DATASETS.build(dict(type=\"WaterTankDataset\", root=root))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "for output in dataloader:\n",
    "    print(output.get('img').shape)\n",
    "    print(output.get('mask').shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from mmengine.optim import (OptimWrapper, OptimWrapperDict, _ParamScheduler,\n",
    "                            build_optim_wrapper)\n",
    "# from mmengine.registry import OPTIM_WRAPPERS\n",
    "from tools.runner import Runner\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 3, 3)\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "\n",
    "optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "optim_wrapper = dict(type='OptimWrapper', optimizer=optimizer, clip_grad=None)\n",
    "param_scheduler = [\n",
    "    dict(\n",
    "        type='LinearLR', start_factor=1e-6, by_epoch=False, begin=0, end=1500),\n",
    "    dict(\n",
    "        type='PolyLR',\n",
    "        eta_min=0.0,\n",
    "        power=1.0,\n",
    "        begin=1500,\n",
    "        end=160000,\n",
    "        by_epoch=False,\n",
    "    )\n",
    "]\n",
    "\n",
    "optim = Runner._build_optimizer('self', model, copy.deepcopy(optim_wrapper))\n",
    "scheduler = Runner._build_param_scheduler('self', optim, copy.deepcopy(param_scheduler))\n",
    "\n",
    "\"\"\" 测试 \"\"\"\n",
    "# for i in range(3000):\n",
    "#     # optim.step()\n",
    "#     for _scheduler in scheduler:\n",
    "#         _scheduler.step()\n",
    "#     # print(optim.param_groups[0]['lr'])\n",
    "#     plt.plot(i, optim.param_groups[0]['lr'], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mmengine.model.base_module.Sequential'>\n",
      "tensor(1.0742, grad_fn=<MulBackward0>)\n",
      "tensor(2.3245, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss[\u001b[38;5;241m0\u001b[39m](pred, target))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss[\u001b[38;5;241m1\u001b[39m](pred, target) \u001b[38;5;241m+\u001b[39m loss[\u001b[38;5;241m0\u001b[39m](pred, target))\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moptim\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "from base.registry import LOSSES\n",
    "import utils.loos\n",
    "import torch\n",
    "\n",
    "# loss_decode=dict(\n",
    "#             type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)\n",
    "# loss_decode=dict(\n",
    "#             type='DiceLoss', loss_weight=1.0)\n",
    "\n",
    "loss_decode=[\n",
    "        dict(type='CrossEntropyLoss', loss_name='loss_ce', loss_weight=1.0),\n",
    "        dict(type='DiceLoss', loss_name='loss_dice', loss_weight=3.0)\n",
    "    ]\n",
    "loss = LOSSES.build(loss_decode)\n",
    "loss__ = LOSSES.build(loss_decode)\n",
    "print(type(loss__))\n",
    "\n",
    "pred = torch.rand((1, 3, 4, 4), requires_grad=True)\n",
    "target = torch.randint(0, 3, (1, 4, 4))\n",
    "\n",
    "print(loss[0](pred, target))\n",
    "print(loss[1](pred, target) + loss[0](pred, target))\n",
    "print(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240625171727\n"
     ]
    }
   ],
   "source": [
    "# 获取当前时间戳\n",
    "import time\n",
    "# 格式化为年月日\n",
    "import datetime\n",
    "\n",
    "def get_current_time():\n",
    "    return datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%0d%H%M%S')\n",
    "\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from base.registry import MODEL\n",
    "import model as ModelZoo\n",
    "\n",
    "cfg = dict(type='Model', a=1, b=2)\n",
    "model = MODEL.build(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config (path: configs/api_test.py): {'dataset_type': 'WaterTankDataset', 'root': 'data/sonar', 'train_dataset': {'type': 'WaterTankDataset', 'root': 'data/sonar', 'split': 'train'}, 'test_dataset': {'type': 'WaterTankDataset', 'root': 'data/sonar', 'split': 'test'}, 'train_dataloader': {'batch_size': 4, 'num_workers': 4}, 'test_dataloader': {'batch_size': 1, 'num_workers': 1}, 'optimizer': {'type': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'weight_decay': 0.0005}, 'optim_wrapper': {'type': 'OptimWrapper', 'optimizer': {'type': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'weight_decay': 0.0005}, 'clip_grad': None}, 'param_scheduler': [{'type': 'LinearLR', 'start_factor': 1e-06, 'by_epoch': False, 'begin': 0, 'end': 1500}, {'type': 'PolyLR', 'eta_min': 0.0, 'power': 1.0, 'begin': 1500, 'end': 160000, 'by_epoch': False}], 'decode_loss': {'type': 'CrossEntropyLoss', 'use_sigmoid': False, 'loss_weight': 1.0}, 'model_cfg': {'type': 'Model', 'a': 1, 'b': 2}}\n",
      "{'type': 'WaterTankDataset', 'root': 'data/sonar', 'split': 'train'}\n",
      "[{'type': 'LinearLR', 'start_factor': 1e-06, 'by_epoch': False, 'begin': 0, 'end': 1500}, {'type': 'PolyLR', 'eta_min': 0.0, 'power': 1.0, 'begin': 1500, 'end': 160000, 'by_epoch': False}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\GitHubProject\\semantic-segmentation-code\\utils\\loos\\cross_entropy_loss.py:250: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"API Test\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "from mmengine.config import Config, DictAction\n",
    "from mmengine.logging import print_log\n",
    "from tools.runner import Runner\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # load config\n",
    "    cfg = Config.fromfile('configs/api_test.py')\n",
    "    print(cfg)\n",
    "    print(cfg.train_dataset)\n",
    "    print(cfg.param_scheduler)\n",
    "    runner = Runner(cfg=cfg)\n",
    "    # if args.cfg_options is not None:\n",
    "    #     cfg.merge_from_dict(args.cfg_options)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 注意 \"\"\"\n",
    "# Initiate inner count of `optim_wrapper`.\n",
    "self.optim_wrapper.initialize_count_status(\n",
    "    self.model,\n",
    "    self._train_loop.iter,  # type: ignore\n",
    "    self._train_loop.max_iters)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss(avg_non_ignore=False)\n",
      "DiceLoss()\n",
      "{'loss_ce': tensor(1.0806, grad_fn=<MulBackward0>), 'loss_dice': tensor(1.2545, grad_fn=<MulBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 注意: 多损失计算 \"\"\"\n",
    "loss = dict()\n",
    "seg_logits = pred\n",
    "seg_label = target \n",
    "loss_decode = loss__\n",
    "\n",
    "for _decode in loss_decode:\n",
    "    print(_decode)\n",
    "    if _decode.loss_name not in loss:\n",
    "        loss[_decode.loss_name] = _decode(\n",
    "            seg_logits,\n",
    "            seg_label)\n",
    "    else:\n",
    "        loss[_decode.loss_name] += _decode(\n",
    "            seg_logits,\n",
    "            seg_label)\n",
    "        \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('loss', tensor(2.3351, grad_fn=<AddBackward0>)), ('loss_ce', tensor(1.0806, grad_fn=<MeanBackward0>)), ('loss_dice', tensor(1.2545, grad_fn=<MeanBackward0>))])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 更新参数代码 \"\"\"\n",
    "from mmengine.utils import is_list_of\n",
    "from collections import OrderedDict\n",
    "\n",
    "log_vars = []\n",
    "for loss_name, loss_value in loss.items():\n",
    "    if isinstance(loss_value, torch.Tensor):\n",
    "        log_vars.append([loss_name, loss_value.mean()])\n",
    "    elif is_list_of(loss_value, torch.Tensor):\n",
    "        log_vars.append(\n",
    "            [loss_name,\n",
    "                sum(_loss.mean() for _loss in loss_value)])\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            f'{loss_name} is not a tensor or list of tensors')\n",
    "\n",
    "loss = sum(value for key, value in log_vars if 'loss' in key)\n",
    "log_vars.insert(0, ['loss', loss])\n",
    "log_vars = OrderedDict(log_vars)  # type: ignore\n",
    "optim.update_params(loss)\n",
    "print(log_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 320, 3)\n",
      "(480, 320, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os.path as osp\n",
    "\n",
    "img = cv2.imread(osp.join('data\\sonar\\Images\\marine-debris-aris3k-16.png'))\n",
    "mask = cv2.imread(osp.join('data\\sonar\\Masks\\marine-debris-aris3k-16.png'))\n",
    "\n",
    "print(img.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 12, 480, 320])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models.resnet as resnet\n",
    "# from base.registry import MODEL\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, a, b):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet.resnet18()\n",
    "        self.decoder = nn.Conv2d(512, 12, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "        x = self.decoder(x)\n",
    "        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=True)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "x = torch.randn(4, 3, 480, 320).cuda()\n",
    "model = Model(1,2).cuda() # Model(1, 2).cuda()\n",
    "# model.fc = nn.Conv2d(256, 12, 1).cuda()\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sonarseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
